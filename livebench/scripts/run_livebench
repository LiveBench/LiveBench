#!/bin/bash
## Run the livebench benchmark (or the specified subset) using the specified model.
## This script will run gen_api_answer and gen_ground_truth_judgment.
## The question-source argument is optional; if not provided, the questions will be downloaded from huggingface
## Usage: run_livebench <benchmark-path> <model> <question-source>
## Example: run_livebench live_bench/coding gpt-4o-mini jsonl

benchmark=$1
model=$2
question_source=${3:-'huggingface'}

if [ -z "$benchmark" ] || [ -z "$model" ]; then
    echo "Usage: run_livebench <benchmark-path> <model> <optional-question-source>"
    exit 1
fi

echo "Running $benchmark with $model, using $venv"

if [ -z "$question_source" ]; then
    python -u gen_api_answer.py --model $model --bench-name $benchmark
else
    echo "Using question source $question_source"
    python -u gen_api_answer.py --model $model --bench-name $benchmark --question-source $question_source
fi

cd data
path=$(find $benchmark -type f -name "$model.jsonl")

if [ -z "$path" ]; then
    echo "Error: No output file found for $model in $benchmark. Exiting."
    exit 1
fi

if grep -H "ERROR" $path; then
    echo "Error found in gen_api_answer.py output. Exiting."
    exit 1
fi

cd ..

if [ -z "$question_source" ]; then
    python -u gen_ground_truth_judgment.py --model $model --bench-name $benchmark
else
    python -u gen_ground_truth_judgment.py --model $model --bench-name $benchmark --question-source $question_source
fi

echo "Finished running $benchmark with $model"